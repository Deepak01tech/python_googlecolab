{"cells":[{"cell_type":"markdown","id":"051d78e3","metadata":{"id":"051d78e3"},"source":["Write a Python function that takes in a sequence of words as input and uses an\n","LSTM model to generate the next word in the sequence. The function should take\n","into account the previous words in the sequence to predict the next word."]},{"cell_type":"code","execution_count":1,"id":"84dd01ca","metadata":{"id":"84dd01ca","executionInfo":{"status":"ok","timestamp":1700025370209,"user_tz":-330,"elapsed":4767,"user":{"displayName":"Deepak Patel","userId":"09825564434122912210"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import regex as re #regular expression"]},{"cell_type":"markdown","id":"38065671","metadata":{"id":"38065671"},"source":["Prepare your text data and labels.\n","Tokenize the text data and convert it into sequences.\n","Pad the sequences to ensure they have consistent lengths.\n","Create a model using the Sequential API.\n","Add layers to the model, including embedding layers for text representation and LSTM layers for sequence processing.\n","Compile the model with a loss function, optimizer, and evaluation metric.\n","Train the model on your data using the fit method.\n","Evaluate the model's performance on test data."]},{"cell_type":"markdown","id":"76033e0d","metadata":{"id":"76033e0d"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","id":"d01c9d9b","metadata":{"id":"d01c9d9b"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":2,"id":"729b1d45","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"729b1d45","executionInfo":{"status":"error","timestamp":1700025377634,"user_tz":-330,"elapsed":620,"user":{"displayName":"Deepak Patel","userId":"09825564434122912210"}},"outputId":"6ab53be3-c625-4332-f394-127035099bf6"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b91e9fb8fd8a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1. Input Gate: Sigmoid (0,1) which value to let through and Tanh (-1,1) weightage of value(importance) Activation Functions\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["1. Input Gate: Sigmoid (0,1) which value to let through and Tanh (-1,1) weightage of value(importance) Activation Functions\n","2. Forget Gate: details to be discarded from block (sigmoid)\n","3. Output Gate"]},{"cell_type":"code","execution_count":3,"id":"af7afe78","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"af7afe78","executionInfo":{"status":"error","timestamp":1700025386692,"user_tz":-330,"elapsed":533,"user":{"displayName":"Deepak Patel","userId":"09825564434122912210"}},"outputId":"ac244310-697d-4c0b-90c7-e288e00e8179"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4376c445f7e3>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pizza.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_to_sentence_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Tokenize the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-4376c445f7e3>\u001b[0m in \u001b[0;36mfile_to_sentence_list\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfile_to_sentence_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Splitting the text into sentences using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pizza.txt'"]}],"source":["def file_to_sentence_list(file_path):\n","    with open(file_path, 'r') as file:\n","        text = file.read()\n","\n","    # Splitting the text into sentences using\n","    # delimiters like '.', '?', and '!'\n","    sentences = [sentence.strip() for sentence in re.split(\n","        r'(?<=[.!?])\\s+', text) if sentence.strip()]\n","\n","    return sentences\n","\n","file_path = 'pizza.txt'\n","text_data = file_to_sentence_list(file_path)\n","\n","# Tokenize the text data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(text_data)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Create input sequences\n","input_sequences = []\n","for line in text_data:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Pad sequences and split into predictors and label\n","max_sequence_len = max([len(seq) for seq in input_sequences])\n","input_sequences = np.array(pad_sequences(\n","    input_sequences, maxlen=max_sequence_len, padding='pre'))\n","X, y = input_sequences[:, :-1], input_sequences[:, -1]\n","\n","# Convert target data to one-hot encoding\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)"]},{"cell_type":"code","execution_count":4,"id":"2cfb2559","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"2cfb2559","executionInfo":{"status":"error","timestamp":1700025399602,"user_tz":-330,"elapsed":524,"user":{"displayName":"Deepak Patel","userId":"09825564434122912210"}},"outputId":"74b27640-ccff-4998-cd8a-a80bb9ad38d0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2326d97b879c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.add(Embedding(total_words, 10,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     input_length=max_sequence_len-1))\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'total_words' is not defined"]}],"source":["# Define the model\n","model = Sequential()\n","model.add(Embedding(total_words, 10,\n","                    input_length=max_sequence_len-1))\n","model.add(LSTM(128))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":5,"id":"d4245dc0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"d4245dc0","executionInfo":{"status":"error","timestamp":1700025406737,"user_tz":-330,"elapsed":571,"user":{"displayName":"Deepak Patel","userId":"09825564434122912210"}},"outputId":"62983561-6a8f-4caa-bd68-cc6c94ddbf04"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-76f4469c36df>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}],"source":["# Train the model\n","model.fit(X, y, epochs=500, verbose=1)"]},{"cell_type":"code","execution_count":6,"id":"46bdcce7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"46bdcce7","executionInfo":{"status":"error","timestamp":1700025413685,"user_tz":-330,"elapsed":473,"user":{"displayName":"Deepak Patel","userId":"09825564434122912210"}},"outputId":"60eea7eb-2352-47c0-d3dd-c25453c18af0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-288d14ba1a23>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     token_list = pad_sequences(\n\u001b[1;32m      8\u001b[0m         [token_list], maxlen=max_sequence_len-1, padding='pre')\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["# Generate next word predictions\n","seed_text = \"Pizza have different \"\n","next_words = 5\n","\n","for _ in range(next_words):\n","    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","    token_list = pad_sequences(\n","        [token_list], maxlen=max_sequence_len-1, padding='pre')\n","    predicted_probs = model.predict(token_list)\n","    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n","    seed_text += \" \" + predicted_word\n","\n","print(\"Next predicted words:\", seed_text)"]},{"cell_type":"code","execution_count":null,"id":"e2c87055","metadata":{"id":"e2c87055","outputId":"1f591822-b192-4410-fbae-3cc310f89cb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 39, 10)            6870      \n","                                                                 \n"," lstm (LSTM)                 (None, 128)               71168     \n","                                                                 \n"," dense (Dense)               (None, 687)               88623     \n","                                                                 \n","=================================================================\n","Total params: 166661 (651.02 KB)\n","Trainable params: 166661 (651.02 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"a868d5d3","metadata":{"id":"a868d5d3"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"id":"f9c185c6","metadata":{"id":"f9c185c6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7ea1a928","metadata":{"id":"7ea1a928"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}